# -*- coding: utf-8 -*-
"""ajinkya-slape-19-01-26-1-updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LJk_d9JJQ17KjQqwJz0IGCdW7bXpjYDZ
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("patrickfleith/nasa-battery-dataset")

print("Path to dataset files:", path)

import os
import pandas as pd

# ðŸ”¹ Set your folder path containing the 7565 CSV files
folder_path = "/kaggle/input/datasets/patrickfleith/nasa-battery-dataset/cleaned_dataset/data"

# Lists to store file paths
type1_files = []
type2_files = []

# Loop through all CSV files in folder
for file in os.listdir(folder_path):
    if file.endswith(".csv"):
        file_path = os.path.join(folder_path, file)
        try:
            # Read only first row to check columns
            df = pd.read_csv(file_path, nrows=1)
            cols = df.columns.tolist()

            # Detect Type 1 (raw battery data)
            type1_columns = ['Voltage_measured', 'Current_measured', 'Temperature_measured',
                             'Current_load', 'Voltage_load', 'Time']
            type2_columns = ['Sense_current', 'Battery_current', 'Current_ratio',
                             'Battery_impedance', 'Rectified_Impedance']

            if all(col in cols for col in type1_columns):
                type1_files.append(file_path)
            elif all(col in cols for col in type2_columns):
                type2_files.append(file_path)
            else:
                # If file doesn't match exactly, ignore or classify as unknown
                pass

        except Exception as e:
            print(f"Error reading {file}: {e}")

# âœ… Results
print(f"Total Type 1 files (usable for SOH): {len(type1_files)}")
print(f"Total Type 2 files (impedance): {len(type2_files)}")

# Optional: Save file paths to text files
with open("type1_files.txt", "w") as f:
    for path in type1_files:
        f.write(path + "\n")

with open("type2_files.txt", "w") as f:
    for path in type2_files:
        f.write(path + "\n")

df = pd.read_csv(type1_files[0])

# Keep discharge only
df = df[df['Current_measured'] < 0]

# Remove tiny current noise
df = df[df['Current_measured'] < -0.05]

df['dt'] = df['Time'].diff().fillna(0)

df['Capacity_Ah'] = -(
    df['Current_measured'] * df['dt']
).cumsum() / 3600

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.plot(df['Capacity_Ah'], df['Voltage_measured'], linewidth=2)
plt.xlabel("Capacity (Ah)")
plt.ylabel("Voltage (V)")
plt.title("Discharge Curve (Voltage vs Capacity)")
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv(type1_files[0])
df = df[df['Current_measured'] < -0.05]  # discharge only

plt.figure(figsize=(8,5))
plt.hist(df['Voltage_measured'], bins=50, color='steelblue', edgecolor='black')
plt.xlabel("Voltage (V)")
plt.ylabel("Frequency")
plt.title("Voltage Distribution (Discharge)")
plt.grid(True)
plt.show()

plt.figure(figsize=(8,5))
plt.hist(df['Temperature_measured'], bins=40, color='orange', edgecolor='black')
plt.xlabel("Temperature (Â°C)")
plt.ylabel("Frequency")
plt.title("Temperature Distribution During Discharge")
plt.grid(True)
plt.show()

capacities = []

for file_path in type1_files[:40]:
    df = pd.read_csv(file_path)
    ddf = df[df['Current_measured'] < -0.05]
    ddf['dt'] = ddf['Time'].diff().fillna(0)
    cap = -(ddf['Current_measured'] * ddf['dt']).sum() / 3600
    capacities.append(cap)

plt.figure(figsize=(12,5))
plt.bar(range(len(capacities)), capacities, color='green')
plt.xlabel("Cycle Number")
plt.ylabel("Capacity (Ah)")
plt.title("Capacity Degradation Across Cycles")
plt.grid(axis='y')
plt.show()

plt.figure(figsize=(8,5))
plt.hist(capacities, bins=20, color='purple', edgecolor='black')
plt.xlabel("Capacity (Ah)")
plt.ylabel("Count")
plt.title("Capacity Distribution Across Cycles")
plt.grid(True)
plt.show()

avg_voltage = []

for file_path in type1_files[:40]:
    df = pd.read_csv(file_path)
    ddf = df[df['Current_measured'] < -0.05]
    avg_voltage.append(ddf['Voltage_measured'].mean())

plt.figure(figsize=(12,5))
plt.bar(range(len(avg_voltage)), avg_voltage, color='dodgerblue')
plt.xlabel("Cycle Number")
plt.ylabel("Average Voltage (V)")
plt.title("Average Discharge Voltage per Cycle")
plt.grid(axis='y')
plt.show()

initial_capacity = capacities[0]
soh = [(cap / initial_capacity) * 100 for cap in capacities]

plt.figure(figsize=(12,5))
plt.bar(range(len(soh)), soh, color='teal')
plt.xlabel("Cycle Number")
plt.ylabel("SOH (%)")
plt.title("State of Health (SOH) Distribution")
plt.ylim(80, 105)
plt.grid(axis='y')
plt.show()

plt.figure(figsize=(6,5))
plt.boxplot(capacities)
plt.ylabel("Capacity (Ah)")
plt.title("Capacity Variation & Outliers")
plt.grid(True)
plt.show()

import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Bidirectional, LSTM
from tensorflow.keras.layers import Dense, Dropout, Layer
from tensorflow.keras.optimizers import Adam
import tensorflow.keras.backend as K
import random

# -----------------------
# 1ï¸âƒ£ Load & merge CSVs
# -----------------------
all_files = type1_files

df_list = []
for file in all_files:
    try:
        df_temp = pd.read_csv(file)
        df_list.append(df_temp)
    except Exception as e:
        print(f"Error reading {file}: {e}")

df = pd.concat(df_list, ignore_index=True)
df.drop_duplicates(inplace=True)
df.reset_index(drop=True, inplace=True)

numeric_cols = df.select_dtypes(include='number').columns
df[numeric_cols] = df[numeric_cols].fillna(0)

# -----------------------
# 2ï¸âƒ£ Compute SOH per cycle (UNCHANGED)
# -----------------------
df['cycle_id'] = (df['Time'].diff() < 0).cumsum()
df['dt'] = df['Time'].diff().fillna(0).clip(lower=0)
df['discharge_current'] = df['Current_measured'].where(df['Current_measured'] < 0, 0)
df['Capacity_Ah'] = (df['discharge_current'].abs() * df['dt']).cumsum() / 3600

cycle_capacity = df.groupby('cycle_id')['Capacity_Ah'].max()
rated_capacity = cycle_capacity.max()
cycle_soh = cycle_capacity / rated_capacity

df['SOH'] = df['cycle_id'].map(cycle_soh).clip(0, 1)

# -----------------------
# 3ï¸âƒ£ Aggregate features per cycle
# -----------------------
agg_df = df.groupby('cycle_id').agg({
    'Voltage_measured': ['min','max','mean','std'],
    'Current_measured': ['min','max','mean','std'],
    'Temperature_measured': ['min','max','mean','std'],
    'Current_load': ['mean'],
    'Voltage_load': ['mean'],
    'Capacity_Ah': ['max']
})

agg_df.columns = ['_'.join(col) for col in agg_df.columns]
agg_df['SOH'] = cycle_soh.values

# -----------------------
# ðŸ”µ 3.1ï¸âƒ£ Compute RUL (NEW)
# -----------------------
total_cycles = len(agg_df)
agg_df['RUL'] = total_cycles - agg_df.index - 1

# -----------------------
# 4ï¸âƒ£ Features & targets
# -----------------------
ALL_FEATURES = [c for c in agg_df.columns if c not in ['SOH', 'RUL']]

X = agg_df[ALL_FEATURES].values
y_soh = agg_df['SOH'].values
y_rul = agg_df['RUL'].values

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
X_scaled = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))

# -----------------------
# 5ï¸âƒ£ Attention Layer (UNCHANGED)
# -----------------------
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight",
                                 shape=(input_shape[-1], 1),
                                 initializer="normal")
        self.b = self.add_weight(name="att_bias",
                                 shape=(input_shape[1], 1),
                                 initializer="zeros")
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = x * a
        return K.sum(output, axis=1)

# -----------------------
# 6ï¸âƒ£ Model Builder (UNCHANGED)
# -----------------------
def build_model(input_shape):
    inp = Input(shape=input_shape)
    x = Conv1D(32, 1, activation='relu')(inp)
    x = MaxPooling1D(1)(x)
    x = Bidirectional(LSTM(32, return_sequences=True))(x)
    x = Attention()(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.2)(x)
    out = Dense(1)(x)
    model = Model(inp, out)
    model.compile(optimizer=Adam(1e-3), loss='mse')
    return model

# -----------------------
# 7ï¸âƒ£ Train FINAL SOH model (UNCHANGED)
# -----------------------
X_tr_soh, X_te_soh, y_tr_soh, y_te_soh = train_test_split(
    X_scaled, y_soh, test_size=0.2, random_state=42
)

final_model_soh = build_model((1, X_scaled.shape[2]))
final_model_soh.fit(X_tr_soh, y_tr_soh,
                    epochs=50,
                    batch_size=16,
                    validation_split=0.1)

y_pred_soh = final_model_soh.predict(X_te_soh).flatten()

print("\n===== SOH RESULTS =====")
print("R2:", r2_score(y_te_soh, y_pred_soh))
print("RMSE:", np.sqrt(mean_squared_error(y_te_soh, y_pred_soh)))
print("MAE:", mean_absolute_error(y_te_soh, y_pred_soh))
print("MAPE:", mean_absolute_percentage_error(y_te_soh, y_pred_soh))

# -----------------------
# ðŸ”´ 8ï¸âƒ£ Train NEW RUL model
# -----------------------
X_tr_rul, X_te_rul, y_tr_rul, y_te_rul = train_test_split(
    X_scaled, y_rul, test_size=0.2, random_state=42
)

final_model_rul = build_model((1, X_scaled.shape[2]))
final_model_rul.fit(X_tr_rul, y_tr_rul,
                    epochs=50,
                    batch_size=16,
                    validation_split=0.1)

y_pred_rul = final_model_rul.predict(X_te_rul).flatten()

print("\n===== RUL RESULTS =====")
print("R2:", r2_score(y_te_rul, y_pred_rul))
print("RMSE:", np.sqrt(mean_squared_error(y_te_rul, y_pred_rul)))
print("MAE:", mean_absolute_error(y_te_rul, y_pred_rul))
print("MAPE:", mean_absolute_percentage_error(y_te_rul, y_pred_rul))

"""# **BaseLine Models**"""

# Use same split for fairness
X_tr_soh, X_te_soh, y_tr_soh, y_te_soh = train_test_split(
    X_scaled, y_soh, test_size=0.2, random_state=42
)

X_tr_flat = X_tr_soh.reshape(X_tr_soh.shape[0], -1)
X_te_flat = X_te_soh.reshape(X_te_soh.shape[0], -1)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import numpy as np

# Train
lr_model = LinearRegression()
lr_model.fit(X_tr_flat, y_tr_soh)

# Predict
y_pred_lr = lr_model.predict(X_te_flat)

# Metrics
print("\n===== Linear Regression (SOH) =====")
print("R2:", r2_score(y_te_soh, y_pred_lr))
print("RMSE:", np.sqrt(mean_squared_error(y_te_soh, y_pred_lr)))
print("MAE:", mean_absolute_error(y_te_soh, y_pred_lr))
print("MAPE:", mean_absolute_percentage_error(y_te_soh, y_pred_lr))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

dnn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_tr_flat.shape[1],)),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(1)
])

dnn_model.compile(optimizer=Adam(1e-3), loss='mse')

dnn_model.fit(
    X_tr_flat, y_tr_soh,
    epochs=50,
    batch_size=16,
    validation_split=0.1,
    verbose=1
)

y_pred_dnn = dnn_model.predict(X_te_flat).flatten()

print("\n===== DNN (SOH) =====")
print("R2:", r2_score(y_te_soh, y_pred_dnn))
print("RMSE:", np.sqrt(mean_squared_error(y_te_soh, y_pred_dnn)))
print("MAE:", mean_absolute_error(y_te_soh, y_pred_dnn))
print("MAPE:", mean_absolute_percentage_error(y_te_soh, y_pred_dnn))

from tensorflow.keras.layers import GRU

gru_model = Sequential([
    GRU(32, return_sequences=False, input_shape=(1, X_scaled.shape[2])),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1)
])

gru_model.compile(optimizer=Adam(1e-3), loss='mse')

gru_model.fit(
    X_tr_soh, y_tr_soh,
    epochs=50,
    batch_size=16,
    validation_split=0.1,
    verbose=1
)

y_pred_gru = gru_model.predict(X_te_soh).flatten()

print("\n===== GRU (SOH) =====")
print("R2:", r2_score(y_te_soh, y_pred_gru))
print("RMSE:", np.sqrt(mean_squared_error(y_te_soh, y_pred_gru)))
print("MAE:", mean_absolute_error(y_te_soh, y_pred_gru))
print("MAPE:", mean_absolute_percentage_error(y_te_soh, y_pred_gru))

from tensorflow.keras.layers import LSTM

lstm_model = Sequential([
    LSTM(32, return_sequences=False, input_shape=(1, X_scaled.shape[2])),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1)
])

lstm_model.compile(optimizer=Adam(1e-3), loss='mse')

lstm_model.fit(
    X_tr_soh, y_tr_soh,
    epochs=50,
    batch_size=16,
    validation_split=0.1,
    verbose=1
)

y_pred_lstm = lstm_model.predict(X_te_soh).flatten()

print("\n===== LSTM (SOH) =====")
print("R2:", r2_score(y_te_soh, y_pred_lstm))
print("RMSE:", np.sqrt(mean_squared_error(y_te_soh, y_pred_lstm)))
print("MAE:", mean_absolute_error(y_te_soh, y_pred_lstm))
print("MAPE:", mean_absolute_percentage_error(y_te_soh, y_pred_lstm))